{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtUhafGE3HvcyuaTrneooj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/univ-3360-vu-smartcities/clustering-demo/blob/master/Clustering_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNtcAUL6JuX6",
        "colab_type": "text"
      },
      "source": [
        "# Clustering Algorithms\n",
        "\n",
        "In this example, we will be looking at how to implement k-means clustering using scikit-learn, and some of the advantages and disadvantages of this algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwVeAP4XJDxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmYI2mW_KAM6",
        "colab_type": "text"
      },
      "source": [
        "# Data Import\n",
        "\n",
        "For this example, we will use a dataset of customer spending at a retail mall. The data includes demographic data about customers and a spending score representing how much money they spent in their combined trip to the mall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciUlV1PYJ9Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"https://raw.githubusercontent.com/univ-3360-vu-smartcities/example-notebooks/master/datasets/Mall_Customers.csv\"\n",
        "mall_data = pd.read_csv(data_path)\n",
        "mall_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DRaLatiLBdb",
        "colab_type": "text"
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "We can plot some of this data to explore replationships between variables. First, we can start with histograms to get an overall sense of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbzskn1AK8n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mall_data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize=(10,5))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "609AF3R3VAKj",
        "colab_type": "text"
      },
      "source": [
        "We can also plot histograms for subsets of data to get an idea of how certain variables affect others. For example, lets plot histograms divided into male shoppers and female shoppers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn9PlyZrLb6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mall_data[mall_data['Gender']=='Male'][['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize=(10,5))\n",
        "plt.suptitle(\"Male Shopping Histograms\")\n",
        "mall_data[mall_data['Gender']=='Female'][['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].hist(figsize=(10,5))\n",
        "plt.suptitle(\"Female Shopping Histograms\")\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcnIPe5SUtS-",
        "colab_type": "text"
      },
      "source": [
        "We can also plot features against one another to start seeing the formation of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U88ZEteKM1Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mall_data.plot(x='Annual Income (k$)', y='Spending Score (1-100)', kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDSyKk3aUyqb",
        "colab_type": "text"
      },
      "source": [
        "Although, clusters are not as obvious for other pairs of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTyj8AeiNsdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mall_data.plot(x='Age', y='Spending Score (1-100)', kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3080Xns1OUWD",
        "colab_type": "text"
      },
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "Now that we have a cursory understanding of the data, we can begin to cluster it. We will use scikit-learn to easily implement K-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A30Va67-ONgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v93aZW4PP_8z",
        "colab_type": "text"
      },
      "source": [
        "## Simple K-Means\n",
        "\n",
        "As a simple example, let us try to cluster two variables at once. For example, lets take annual income and spending score. From our earlier plotting, it seems fairly clear that these two variables have 5 clusters. So, lets train the K-means algorithm specifying k=5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks9cFb5SOfCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means = KMeans(n_clusters=5, random_state=4)\n",
        "k_means.fit(mall_data[['Annual Income (k$)','Spending Score (1-100)']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUHi5UEVwH3",
        "colab_type": "text"
      },
      "source": [
        "Once the model is trained, we can print the labels and the centroids for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GvZNDf0OyUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means_labels = k_means.labels_\n",
        "print(\"Labels: \", k_means_labels)\n",
        "print()\n",
        "\n",
        "k_means_centroids = k_means.cluster_centers_\n",
        "print(\"Centroids: \\n\", k_means_centroids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3gADc0V2hz",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the data color coodinated with the determined cluster. This will allow us to see if K-means matched our intuition of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMp6_YNJO4JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "unique_labels = set(k_means_labels)\n",
        "core_samples_mask = np.zeros_like(k_means_labels, dtype=bool)\n",
        "colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "for k in unique_labels:\n",
        "    class_member_mask = (k_means_labels == k)\n",
        "    xy = mall_data[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy['Annual Income (k$)'], xy['Spending Score (1-100)'], color=colors[k%7])\n",
        "\n",
        "plt.title(\"K-Means Generated Clusters for Mall Data and K=5\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Spending Score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5cMqTPIQCrh",
        "colab_type": "text"
      },
      "source": [
        "## Effect of Wrong K Parameter\n",
        "\n",
        "One of the main issues with the K-means algorithm is that the number of clusters must be known before running the algorithm. But what happens if we run K-means with the wrong number of clusters? Let's try the same data but using 3 clusters instead of 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaQIKi8nPJv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means = KMeans(n_clusters=3, random_state=4)\n",
        "k_means.fit(mall_data[['Annual Income (k$)','Spending Score (1-100)']])\n",
        "\n",
        "k_means_labels = k_means.labels_\n",
        "k_means_centroids = k_means.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhAKtGDFQJ-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "unique_labels = set(k_means_labels)\n",
        "core_samples_mask = np.zeros_like(k_means_labels, dtype=bool)\n",
        "colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "for k in unique_labels:\n",
        "    class_member_mask = (k_means_labels == k)\n",
        "    xy = mall_data[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy['Annual Income (k$)'], xy['Spending Score (1-100)'], color=colors[k%7])\n",
        "\n",
        "plt.title(\"K-Means Generated Clusters for Mall Data and K=3\")\n",
        "plt.xlabel(\"Annual Income\")\n",
        "plt.ylabel(\"Spending Score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLpbtzLfWe23",
        "colab_type": "text"
      },
      "source": [
        "As you can see, while the clustering might not be the ideal clustering of the data, K-means still provided a somewhat sensible interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmO5ZxENQoho",
        "colab_type": "text"
      },
      "source": [
        "## Unknown K Value\n",
        "\n",
        "So how do we actually determine the value of K? For the above two features, it was fairly clear by intuition, but what happens if it is not as clear? For example, consider the features of age and spending score. There is not as immediately obvious of a relationship.\n",
        "\n",
        "As a first test, lets use the same k=5 that we used on the other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M0u6IUhQM0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means = KMeans(n_clusters=5, random_state=4)\n",
        "k_means.fit(mall_data[['Age','Spending Score (1-100)']])\n",
        "\n",
        "k_means_labels = k_means.labels_\n",
        "k_means_centroids = k_means.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwj7zHTKQ46w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "unique_labels = set(k_means_labels)\n",
        "core_samples_mask = np.zeros_like(k_means_labels, dtype=bool)\n",
        "colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "for k in unique_labels:\n",
        "    class_member_mask = (k_means_labels == k)\n",
        "    xy = mall_data[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy['Age'], xy['Spending Score (1-100)'], color=colors[k%7])\n",
        "\n",
        "plt.title(\"K-Means Generated Clusters for Mall Data and K=5\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Spending Score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW95YUWfXGnC",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this produces an arguably sensible clustering of the data. However, we really just guessed here. Instead, we would like to determine the value of K from the data itself. This is not an easy task, but there are some heuristics we can use.\n",
        "\n",
        "The overall goal of clustering is to minimize the within cluster distance and simultaneously maximize the between cluster distance. There are several metrics that we can use to measure this. One such metric is *inertia* which measures the sum of squared distances of samples to their closest cluster center. If we run K-means for a variety of k values, we can use inertia to get a better view of how the clustering is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKz5jDgsQ7I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_values = [1,2,3,4,5,6,7,8,9,10]\n",
        "inertia_values = []\n",
        "\n",
        "for k in k_values:\n",
        "  k_means = KMeans(n_clusters=k, random_state=4)\n",
        "  k_means.fit(mall_data[['Age','Spending Score (1-100)']])\n",
        "  inertia_values.append(k_means.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygq99ChkSsKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(k_values, inertia_values)\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia Value\")\n",
        "plt.title(\"Trade Off Curve for Age/Spending Clustering\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPe0nzthX_yQ",
        "colab_type": "text"
      },
      "source": [
        "So you can see that the inertia approches zero as the number of clusters increases, which is what we would expect. However, in order to make our clustering more generalizable and actually find patterns, we cannot just increase k indefinitely. One technique for choosing k is to pick the \"knee of the curve\", also known as the trade-off point, of this inertia curve. For this data, we can see that this occurs around K=3.\n",
        "\n",
        "So, let's try plotting the clustering of K=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYqMLKt5TD2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means = KMeans(n_clusters=3, random_state=4)\n",
        "k_means.fit(mall_data[['Age','Spending Score (1-100)']])\n",
        "\n",
        "k_means_labels = k_means.labels_\n",
        "k_means_centroids = k_means.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DoVjj-DTzJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "unique_labels = set(k_means_labels)\n",
        "core_samples_mask = np.zeros_like(k_means_labels, dtype=bool)\n",
        "colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "for k in unique_labels:\n",
        "    class_member_mask = (k_means_labels == k)\n",
        "    xy = mall_data[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy['Age'], xy['Spending Score (1-100)'], color=colors[k%7])\n",
        "\n",
        "plt.title(\"K-Means Generated Clusters for Mall Data and K=4\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Spending Score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lvw-VVoYlP6",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this revealed some kind of underlying structure to this data that was not immediately obvious at first glace. This technique of using the trade-off curve to determine the value of K is quite powerfull and very common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV0jFzeZY9fw",
        "colab_type": "text"
      },
      "source": [
        "# Non-Globular Clusters\n",
        "\n",
        "So we have seen that K-means is extremely powerful if the correct value of K is chosen. However, if it universal? Can we cluster any data using K-means?\n",
        "\n",
        "As it turns out, there are certain types of data that are not ammenable to K-means. For an example, lets take a look at the moon data from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogVR-Mi7Zdft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi4SLrv4T3YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_moon_data, _ = make_moons(n_samples=1500, noise=0.05, random_state=1)\n",
        "\n",
        "plt.scatter(X_moon_data[:,0], X_moon_data[:,1], s=6)\n",
        "plt.title(\"Moon Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmx6UsKIZsch",
        "colab_type": "text"
      },
      "source": [
        "Clearly, there are two distinct clusters in this dataset. So, let's try running K-means with K=2 to see how it clusters the moon data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9z0G_soZZh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_means_moon = KMeans(n_clusters=2)\n",
        "k_means_moon.fit(X_moon_data)\n",
        "\n",
        "k_means_labels = k_means_moon.labels_\n",
        "k_means_centroids = k_means_moon.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdmESRwZ9PZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_labels = set(k_means_labels)\n",
        "core_samples_mask = np.zeros_like(k_means_labels, dtype=bool)\n",
        "colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "for k in unique_labels:\n",
        "    class_member_mask = (k_means_labels == k)\n",
        "    xy = X_moon_data[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy[:,0], xy[:,1], s=6, color=colors[k%7])\n",
        "\n",
        "plt.title(\"K-Means Generated Clusters for Moon Data\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zynyzxEKaSS8",
        "colab_type": "text"
      },
      "source": [
        "So we can see that a somewhat unexpected result occurs. K-means was not able to generate clusters for the moons and instead generated two clusters that both overlapped the actual clusters. This happens because the ordinary version of K-means generates globular shaped clusters only. So, for clusters which do not follow this patter, K-means is not a good choice. \n",
        "\n",
        "For data that has these kinds of non-globular clusters, we should instead use a different clustering algorithm such as density-based or hierarchical clustering. We will learn more about these techniques in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQWPk44eaM7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}